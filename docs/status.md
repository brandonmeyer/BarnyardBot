---
layout: default
title: Status
---

video here  

## Project Summary
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gathering resources from cows and sheep in *Minecraft* can be tedious. When a player needs bulk buckets of milk or stacks of wool, they have to walk to the animal pen and manually gather all of the resources. If a player only wants one color of wool, they have to chase after the sheep in their pen and find the right colors. With cooldowns, the player could also find themself waiting for more milk or for wool to grow back. This is time consuming, and prevents the player from quickly accessing these items for projects like baking, healing status effects, creating colorful builds, or making beds to sleep through the night. Animal-AI fixes this problem by navigating throughout an animal pen and automatically gathering these resources for the player. It takes the agent's current target as input and output the correct action to harvest the current animal's resources. For example, seeing a cow promts the agent to use a bucket and seeing a sheep prompts the agent to use shears. BarynardBot will only harvest resources that the player specifies. If the agent is looking at the wrong animal, the wrong color sheep, or no animal at all, it will walk around the pen until it finds what it its looking for. The goal of this project is to use *Malmo* and *RLlib* for animal resource gathering automation, and give the player more free time to explore their creativity in *Minecraft*.  

## Approach
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot uses reenforcement learning to adjust agent behavior for harvesting resources. The Proximal Policy Optimization (PPO) algorithm was selected from *RLlib* for this project. This algorithm works well for the intended purpose because the action space is discrete, the observation space is concise, the learning is on-policy, and there are no hyperparamenters needed. This algorithm was selected using a [reenforcement learning algorithm flowchart](https://static.us.edusercontent.com/files/eS20DbiGQfi4P2skbCN9WYeD). The update equation for this algorithm is as follows:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reenforcement learning works well for this project because the agent needs to learn which animals to interact with from the observation space. The observation space is represented by a tuple of information (x, y), where x is the current target of the agent and y is the current tool being used by the agent. The x values are 0 for no animal, 1 for cow, 2 for red sheep, and 3 for blue sheep. The target information is obtained through *Malmo*'s observation JSON. The y values are 0 for shears and 1 for bucket, giving a total of 8 possible animal/tool combinations. With this observation space, the agent will always know what it is looking at and what it is holding. The discrete action space has 7 options: move forward, move back, strafe left, strafe right, hold item 1, hold item 2, or use item. These actions are enough for the agent to move around the animal pen, switch tools, and use a tool on the target. The agent spawns in a pen surrounded by fence, which prevents it from losing the animals.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The rewards are based on the current desires of the player. By default, the agent will harvest milk, blue wool, and red wool, all with a +1 reward. However, if the player only wants milk, the reward for milking a cow stays +1 and the reward for shearing any color sheep changes to -1. Adjusting the reward function to penalize harvesting the wrong resource forces the agent to learn what resource is desired, and stay away from the other resources. The total reward $$T$$ for a step is determined by $$T = m + r + b$$, were $$m$$ is the number of milk buckets collected, $$r$$ is the number of red wool collected, and $$b$$ is the number of blue wool collected. Typically, the agent only collects one of these resources per step, making the other two variables 0. However, it is possible to collect wool while doing another action if the agent didn't pick up wool from the ground previously. Upon milking a cow, all cows are shuffled to a new location. This makes it harder for the agent because it cannot infinitely milk the same cow for maximum reward. This is not an issue for the sheep because sheep must grow their wool back before being sheared again.

## Evaluation
eval here  

## Remaining Goals and Challenges
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Over the next few weeks, our goal is to implement user input through *Minecraft* to set the target item ratios. Currently, if a player wants milk only, wool only, or a specific color of wool only, the user input is done outside of *Minecraft*. While this is working for the intended purpose, the goal is to get this function working through a new *Malmo* observation such as chat commands or block placement. If the player types "0:1:1" in chat, for example, BarnyardBot will harvest no milk, and equal parts blue and red wool. For block placement, the agent would observe a designated grid that represents the ratio. For example, a 10x3 grid where each column represets a resource. One block in each column would mean a 1:1:1 ratio, five blocks in the middle column would mean a 1:5:1 ratio, or no blocks in the last two columns would mean a 1:0:0 ratio. This would be more challenging to implement than ratios through chat, since the agent has to learn how the block ratios work on top of learning to collect resources. In the current state of the project, the player has to update a line in a file whenever they want to update the ratio.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It would also be interesting to try a hand-coded policy since the observation space is not overwhelmingly large. Comparing the results from a hand-coded policy to that of the current *RLlib* algorithm would be a good comparison for the final evaluation. The current *RLlib* algorithm performs well, but writing a policy designed specifically for BarnyardBot could potentially increase the performance.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Future challenges are mainly centered around player input for ratios. If the approach of using blocks to represent ratios is taken, the agent will need to spend more time learning for each run. This might also increase the time needed for each mission, because the agent needs enough time to learn from the increased observation space. One of the early challenges was having an observation space that was far too large, and it is possible for the grid to cause the same issue. A possible solution to this would be decreasing the size of the grid, but that limits the number of ratios that can be selected. If the chat option is taken for ratios, the challenge will be parsing out chat messages that are not related to adjusting BarnyardBot's policy. A possible solution would be adding a phrase to the start of every message, or whispering directly to BarnyardBot instead of typing in chat. Adding a new observation space for ratios will be time consuming because the agent will take longer to learn with each new option.

## Resources used
resources here  
