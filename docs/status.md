---
layout: default
title: Status
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/sVeyd6rsdJo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

## Project Summary
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gathering resources from cows and sheep in *Minecraft* can be tedious. When a player needs bulk buckets of milk or stacks of wool, they have to walk to the animal pen and manually gather all of the resources. If a player only wants one color of wool, they have to chase after the sheep in their pen and find the right colors. With cooldowns, the player could also find themself waiting for more milk or for wool to grow back. This is time consuming, and prevents the player from quickly accessing these items for projects like baking, healing status effects, creating colorful builds, or making beds to sleep through the night. BarnyardBot fixes this problem by navigating throughout an animal pen and automatically gathering these resources for the player. It takes the agent's current target as input and output the correct action to harvest the current animal's resources. For example, seeing a cow promts the agent to use a bucket and seeing a sheep prompts the agent to use shears. BarynardBot will only harvest resources that the player specifies. If the agent is looking at the wrong animal, the wrong color sheep, or no animal at all, it will walk around the pen until it finds what it its looking for. The goal of this project is to use *Malmo* and *RLlib* for animal resource gathering automation, and give the player more free time to explore their creativity in *Minecraft*.  

## Approach
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot uses reinforcement learning to adjust agent behavior for harvesting resources. Reinforcement learning is done with *RLlib* and *Gym* to enable repeat missions and adjust the learning policy. The Proximal Policy Optimization (PPO) algorithm was selected from *RLlib* for this project. This algorithm works well for the intended purpose because the action space is discrete, the observation space is concise, the learning is on-policy, and the space is not not robust to hyperparameters. This algorithm was selected using a [reinforcement learning algorithm flowchart](https://static.us.edusercontent.com/files/eS20DbiGQfi4P2skbCN9WYeD). The update equation for the off-the-shelf PPO algorithm is based on the ratio of old and new policies, and is roughly as follows:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$L^{CLIP}(\theta)=\hat{E_t}[min(r_t(\theta)\hat{A_t},clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})]$$&nbsp;&nbsp;&nbsp;[(source)](https://blogs.oracle.com/ai-and-datascience/post/reinforcement-learning-proximal-policy-optimization-ppo)\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reinforcement learning works well for this project because the agent needs to learn which animals in the animal pen to interact with from the observation space. The observation space is represented by a tuple of information (x, y), where x is the current target of the agent and y is the current tool being used by the agent. The x values are 0 for no animal, 1 for cow, 2 for red sheep, and 3 for blue sheep. The target information is obtained through *Malmo*'s observation JSON. The y values are 0 for shears and 1 for bucket, giving a total of 8 possible animal/tool combinations. With this observation space, the agent will always know what it is looking at and what it is holding. The discrete action space has 7 options: move forward, move back, strafe left, strafe right, hold item 1, hold item 2, or use item. These actions are enough for the agent to move around the animal pen, switch tools, and use a tool on the target. The agent spawns in a pen surrounded by fence, which prevents it from losing the animals.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The rewards are based on the current desires of the player. By default, the agent will harvest milk, blue wool, and red wool, all with a +1 reward. However, if the player only wants milk, the reward for milking a cow stays +1 and the reward for shearing any color sheep changes to -1. Adjusting the reward function to penalize harvesting the wrong resource forces the agent to learn what resource is desired, and stay away from the other resources. The total reward $$T$$ for a step is determined by $$T = r_m * m + r_r * w_r + r_b * w_b$$, were $$r_m$$ is the reward for collecting milk, $$m$$ is the number of milk buckets collected, $$r_r$$ is the reward for collecting red wool, $$w_r$$ is the number of red wool collected, $$r_b$$ is the reward for collecting blue wool, and $$w_b$$ is the number of blue wool collected. Typically, the agent only collects one of these resources per step, making the other two terms 0. However, it is possible to collect wool while doing another action if the agent didn't pick up wool from the ground previously. Upon milking a cow, all cows are shuffled to a new location. This makes it harder for the agent because it cannot infinitely milk the same cow for maximum reward. This is not an issue for the sheep because sheep must grow their wool back before being sheared again. The ratios for collecting resources are currently configured by the reward. If the player does not want milk, for example, the milk reward is set to -1. All resources with the same desired ratio are set to the same reward, which can be read from a .txt file so the user can configure them whenever they want.

## Evaluation
### Quantitative
<p float=left>
  <img src="https://user-images.githubusercontent.com/51243475/141702221-252f1a85-ed36-4342-899f-e2ef6647baf4.png" alt="Animals and Resources" width="300" height="220"> 
  <img src="https://user-images.githubusercontent.com/51243475/141702224-9894a05a-2b22-4b2a-b3d3-3541ed599539.png" alt="Animals and Resources" width="300" height="220"> 
  <img src="https://user-images.githubusercontent.com/51243475/141702227-fa5cf5e6-7ab8-4fb0-8e2e-1e9ae1ee0286.png" alt="Animals and Resources" width="300" height="220"> 
</p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The focus quantitative evaluations for this project was the number of resources collected, represented by average reward, for three different ratios. For example, if the agent wanted blue wool and collected 10, it would have a reward of 10. The ratios evaluated for the status report were collecting milk only, collecting red and blue wool only, and collecting as much as possible of each. As can be seen in the figures above, all three reward ratios resulted in a general uptrend in reward per mission over many steps. The three graphs show generally linear growth on average, and none of them appear to have reached a ceiling. One standout in the third graph, with equal points for each resource, is the spike in reward around 7,500 to 8,500 steps. This could be due to the agent temporarily finding a way to earn many points, such as cornering animals, that eventually stopped working and resulted in less points. It is also possible that a few outlier missions with high reward (15+ points) unevenly weighted the average for these steps. Regardless, it can be seen that the overall reward was trending back towards those numbers before the agent was stopped around 22,000 steps. The graphs of average reward per ten steps prove that the PPO algorithm for reinforcement learning was working effectively to teach the agent to gather resources. Positive average slope in all three graphs also means that regardless of the selected ratio (1:0:0, 0:1:1, 1:1:1), the BarnyardBot was working correctly and returning the expected result. When other algorithms were tested, such as S2C and DQN, they did not have similar results and instead reflected no learning. This supoorts that the PPO algorithm is the correct choice for BarnyardBot.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For the first 5000 steps of each run, the average points scored per mission was 0.47 for milk only, 0.22 for wool only, and 2.15 for collecting all resources. A higher average reward for collecting all resources makes sense, because there were no resources that resulted in a negative reward. Over the last 5000 steps of each run, the average was 4.42 for milk only, 2.38 for wool only, and 5.28 for collecting all resources. That makes it +3.95 for collecting milk, +2.16 for collecting wool, and +3.13 for collecting all resources. The positive values for each of these numbers reflect that the learning algorithm has a significant impact on agent performance. Additionally, it can be assumed from the graphs that spending more time training the agent will result in a higher increase in resources collected. If the mission lengths were increased from 30 seconds, the agent would also collect more resources and rewards per mission. The average values 4.42, 2.38, and 5.28 for milk only, wool only, and all resources, mean that if the agent was run for 10 minutes it would collect 88 bucketes of milk if only collecting milk, 48 wool if only collecting wool, or 106 items if collecting everything. These numbers are all useful amounts considering that the player could let the agent do all of the work. However, the number for only collecting wool for 10 minutes would ideally be higher than 48. One cause of this may be that the agent attempts to shear already-sheared sheep. For future versions of the project, one goal could be to increase this number.

### Qualitative
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One of the qualitative evaluations for BarnyardBot is whether or not unnecessary actions are being taken between collecting resources. Based on the numbers from the quantitative section, it can be deduced that the agent takes more unnecessary actions when collecting only wool than it does when collecting only milk. The 'moonshot' goal for this project is to decrease unnecessary actions as much as possible, so this observation suggests that either the agent needs to learn for longer periods of time, or the interaction between the agent and sheep needs to be adjusted. For example, it is possible that the agent is shearing sheep but not collecting some of the wool. When interacting with cows or both cows and sheep, however, the agent is sucessfully minimizing the number of unnecessary actions. This can be attributed to the reinforcement learning algorithm successfully updating its policy based on the reward function. Adjusting the agent-sheep interactions may be useful for improving this evaluation towards the 'moonshot' goal for the final version of the project. Regardless, the numbers provided in the quantitative section still reflect useful amounts of resources for the player with over a stack of milk, just under a stack of wool, or almost two stacks when collecting milk and both colors of wool.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another qualitative evaluation for BarnyardBot was viewing the state/action pairs from the algorithm and deciding whether or not the results are reasonable. Because an off-the-shelf PPO algorithm was used from *RLlib*, we do not have the expected data to make the decision of whether or not the state/action are reasonable. However, this evaluation can also be done by looking at the graphs from the quantitative section. Based on these figures, the policy was providing reasonable state/action pairs when making decisions. If the policy was not providing reasonable pairs, the graphs wouldn't be increasing in value. This means that the PPO algorithm was an effective choice for solving the problem at hand.

## Remaining Goals and Challenges
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Over the next few weeks, our goal is to implement user input through *Minecraft* to set the target item ratios. Currently, if a player wants milk only, wool only, or a specific color of wool only, the user input is done outside of *Minecraft*. While this is working for the intended purpose, the goal is to get this function working through a new *Malmo* observation such as chat commands or block placement. If the player types "0:1:1" in chat, for example, BarnyardBot will harvest no milk, and equal parts blue and red wool. For block placement, the agent would observe a designated grid that represents the ratio. For example, a 10x3 grid where each column represets a resource. One block in each column would mean a 1:1:1 ratio, five blocks in the middle column would mean a 1:5:1 ratio, or no blocks in the last two columns would mean a 1:0:0 ratio. This would be more challenging to implement than ratios through chat, since the agent has to learn how the block ratios work on top of learning to collect resources. In the current state of the project, the player has to update a line in a file whenever they want to update the ratio.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It would also be interesting to try a hand-coded policy since the observation space is not overwhelmingly large. Comparing the results from a hand-coded policy to that of the current *RLlib* algorithm would be a good comparison for the final evaluation. The current *RLlib* algorithm performs well, but writing a policy designed specifically for BarnyardBot could potentially increase the performance.\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Future challenges are mainly centered around player input for ratios. If the approach of using blocks to represent ratios is taken, the agent will need to spend more time learning for each run. This might also increase the time needed for each mission, because the agent needs enough time to learn from the increased observation space. One of the early challenges was having an observation space that was far too large, and it is possible for the grid to cause the same issue. A possible solution to this would be decreasing the size of the grid, but that limits the number of ratios that can be selected. If the chat option is taken for ratios, the challenge will be parsing out chat messages that are not related to adjusting BarnyardBot's policy. A possible solution would be adding a phrase to the start of every message, or whispering directly to BarnyardBot instead of typing in chat. Adding a new observation space for ratios will be time consuming because the agent will take longer to learn with each new option. Additionaly, it is possible for the agent to collect slightly more of one wool than another, because wool drop values are random. A possible solution for this would be adjusting the reward dynamically, so the agent will focus on the color of wool that it has slightly less of.

## Resources used
[RLlib](https://docs.ray.io/en/latest/rllib.html)\
[RLlib PPO](https://docs.ray.io/en/latest/rllib-algorithms.html#ppo)\
[PPO Algorithm Source](https://blogs.oracle.com/ai-and-datascience/post/reinforcement-learning-proximal-policy-optimization-ppo)\
[Gym](https://gym.openai.com/)\
[Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/)\
[Malmo Gitter Chat](https://gitter.im/Microsoft/malmo?at=578aa4fd3cb52e8b24cee1af)\
[Malmo XML](https://microsoft.github.io/malmo/0.21.0/Schemas/MissionHandlers.html)\
[Minecraft](https://www.minecraft.net/en-us/login)\
[Writing 3x3 Letters in Minecraft (For Home Page/Video Image)](https://www.youtube.com/watch?v=vHExVqV-FD8)\
[Reinforcement Learning Algorithm Flowchart](https://static.us.edusercontent.com/files/eS20DbiGQfi4P2skbCN9WYeD)\
[CS175 Assignment 2 for understanding of RLlib/Gym](https://canvas.eee.uci.edu/courses/40175/files/folder/assignment2?preview=16066666)\
[Displaying Images Side by Side in GitHub](https://stackoverflow.com/questions/24319505/how-can-one-display-images-side-by-side-in-a-github-readme-md)
