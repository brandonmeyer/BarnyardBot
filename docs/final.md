---
layout: default
title: Final Report
---
## BarnyardBot Video

## Project Summary
<img src="https://user-images.githubusercontent.com/51243475/144334516-68004b17-8994-4385-a4c4-7a60bf262afb.png" alt="Problem Image" width="425" height="300">  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gathering animal resources is a crucial part of *Minecraft*<sup>1</sup> that can often be tedious. When a player wants milk or wool for baking a cake, healing status effects, making beds, or building a new colorful project, they must find the correct tools to use and then track down the respective animal for harvesting. On average, it takes a minute for a sheep's wool to grow back<sup>2</sup> in *Minecraft*. This means that most of the time, the player must wait for the cooldown to end if they need a specific color of wool from a specific sheep. This makes animal resource harvesting time consuming, and forces the player to sit around and wait instead of exploring the world, mining, or working on a new build.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot solves this problem by automating the animal resource harvesting process. With the power of reinforcement learning, BarnyardBot can navigate throughout the animal pen and harvest resources for the player. The player can specify whether they need milk, a certain color of wool, or a ratio of resources. BaryardBot collects the requested items and gives the player more free time, removing the needs to sit around and wait for cooldowns or chase around animals. This gives the player more time to explore the world, go mining for precious stones, cook food, or build their next work of art.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot takes the agent's current target and the player's requests as input and determines the current best solution for meeting the player's requests. For example, if the player only wants milk and does not want wool, BarnyardBot will find cows to milk and ignore the sheep it comes across. Taking the player's requests is important for solving the animal resource harvesting problem because the player won't always need every resource. BarnyardBot aims to use this information to harvest resources as efficiently and accurately as possible. If the player only needs a certain color of wool for a build, BarnyardBot won't waste time collecting other colors. Whenever the player wants a different resource, they can tell BarnyardBot and it will adjust accordingly. This prevents the player from needing multiple bots to perform a similar task, since BarnyardBot can harvest whatever they need from sheep and cows. The goal of this project is to maximize resource output in *Minecraft* using *Malmo*<sup>3</sup> and *RLlib*<sup>4</sup>, with the output based on the requests of the player.

## Approaches
#### Setup
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot uses reinforcement learning through *RLlib*, *Gym*<sup>5</sup>, and *Malmo* to adjust agent behavior in *Minecraft* for harvesting resources. To set up the environment, we spawn an 11 by 11 block perimeter fence in a flat world, and summon 8 cows, 8 red sheep, and 8 blue sheep. All animals are summoned at random locations inside the pen. We found that summoning the animals through *Minecraft* summon chat commands was better than using the *Malmo* XML<sup>6</sup> because it gave us more freedom for summoning animals at other times and customizing colors and nametags. The agent spawns in the middle of the pen, and has shears in hotbar 1 and a bucket in hotbar 2 so it can interact with the animals. The agent learns in 30 second missions, and the environment resets after each mission. We found this to be effective because it regularly randomizes the locations of the animals around the agent. It also minimizes the chances of the agent picking up wool that it sheared previously and missed, which was important because it encourages the agent to pick up harvested materials to score points. The first challenge we came across when setting up the environment was that harvesting milk with a bucket did not trigger the *Malmo* reward system. To fix this, we used ‘ObservationFromHotbar’ in the *Malmo* XML. Whenever a milk bucket was observed in the agent's hand, we used *Minecraft* chat commands to replace the milk bucket with a regular bucket. We then incremented the total reward by the appropriate amount for milk.  <br/>
#### Actions and Observations
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When creating the first baseline for the project, the most important details were how the agent would perform its actions (the action space) and how the agent would observe its environment (the observation space). We tested two different action spaces: continuous and discrete. For the continuous action space, the agent could turn left or right, walk forwards or backwards, switch between hotbar 1 and 2, and use the current item. For the discrete action space, the agent could move forwards, backwards, left, or right, switch between hotbar 1 and 2, and use the current item. One of the challenges for the discrete action space was that using an item through Malmo didn't work. To fix this, we used both continuous and discrete commands, and used continuous whenever the agent tried to use an item. This was done utilizing the command 'allow-list' and 'deny-list' options in the Malmo XML. One of the challenges of using a continuous use command was that the agent could infinitely milk the same cow when its bucket was replaced to reward for collecting milk. To fix this, we kill and respawn all cows using the ‘/kill @e[type=minecraft:cow]’ chat command.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Overall, we found that the discrete action space with a continuous use command worked best for our project because the agent could quickly navigate the space and locate new animals. With continuous actions, the agent spent a lot of time spinning and had a harder time harvesting resources. Because the animal pen is an 11 by 11 block grid, discrete actions were also useful for ensuring the agent stayed on the grid. Another issue with the continuous action space was the amount of data needed for the agent to learn. Because there are so many possible actions, the agent would need to spend much more time testing and exploring the possibilities, and we found that running the program for several hours was not enough to see improvement.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We tested two different observation spaces: a 5 by 5 block grid of entities around the agent, and a number representing the entity in the line of sight of the agent. For both approaches, air was represented as 0, cows were represented as 1, red sheep were represented as 2, blue sheep were represented as 3, and the agent was represented as 4. For the 5 by 5 grid approach, we used ‘ObservationFromNearbyEntities’ in the *Malmo* XML. This returned the location of all entities, and we created a matrix based on the blocks surrounding the agent. It was important to account for the Agent's YAW and adjust the grid based on the direction it was facing. The main issue for this approach was the large number of possible observation states. Because each of the 25 entries in the matrix had 5 possibilities, there were 5<sup>25</sup> possible states and the agent struggled to learn with any algorithm. Learning over this observation space would take a very long time, as the agent would need to explore all of the different possible states and test every action over these states. Also, because there are so many possible states, some of them would never be encountered after running the program overnight. For the line of sight approach, we used ‘ObservationFromRay’ in the *Malmo* XML. This returned the type of entity in the agent's line of sight, which was converted into a tuple (x,y) where x represented the entity number and y represented the current hotbar slot (1 or 2). This approach has 4 \* 2 possibilities, which was much more manageable for the learning algorithm. Because less data was needed for the learning algorithm to show improvements, we decided to use the line of sight approach for the base observation.  <br/>
#### Ratios
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The base observation space was then expanded to include player input in the form of ratios. These ratios represent what items the player wants the agent to harvest, and are in the format m:r:b where m is the number for milk, r is the number for red wool, and b is the number for blue wool. Each number can be between 0 and 2 inclusive. For example, 1:1:1 means equal resources of each, 1:0:0 means harvest milk and no wool, and 2:1:0 means harvest 2 milk for every red wool, and no blue wool. These numbers are then added to the observation space (one for each item, values 0, 1, or 2), and the reward for collecting each resource is adjusted accordingly. This increased the number of possibilities in the observation space from 4 \* 2 to 4 \* 2 \* 3 \* 3 \* 3, or 216 possibilities. For example, [1,1,0,0,1] means the agent is looking at a cow, holding shears, and the current request is to only collect blue wool. The increase in observation space required more time in the learning process, but increased the complexity of the observations.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Several approaches were taken for implementing ratios into the observation space. The first approach used a .txt file where the player would type the desired ratio of items on line 3. The program would read the .txt file at the start of every mission and adjust the observations and rewards accordingly. This approach was the simplest, as the player just had to open the .txt file and make an adjustment whenever they felt necessary. One of the issues is that the player can cause an error if they are editing the file while the program is trying to read it. To avoid this, the player needs to pay attention to when the mission is resetting.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another approach involved using ‘ObservationFromChat’ in the *Malmo* XML. With this approach, the program reads any chat messages in the format ‘!RATIO m:r:b’ and adjusts the observations and rewards accordingly. This approach is useful because it allows the player to adjust the agent’s current goals from inside of *Minecraft*, rather than having to edit a file in a separate program. One of the issues with this approach was that *Malmo* occasionally misses chat messages, as is explained in the *Malmo* XML. To combat this, the agent sends a chat command saying ‘Ratio m:r:b accepted, and will be used starting in the next mission’ whenever a chat message is successfully read. If the agent doesn’t respond, the player knows that they have to send the chat message again. Sometimes it can take 3 or so tries before the observation is triggered by *Malmo*.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The last approach used ‘ObservationFromGrid’ in the *Malmo* XML. With this approach, the program detects a grid of blocks placed by the player that represent the ratio. Each resource is represented by a different block, and the rewards and observations are adjusted accordingly. One challenge with this method is that the agent is constantly moving, so the observation grid has to be large. To fix this, we parsed the observation grid for the three types of blocks used and entered the appropriate ratio into the observation. Ultimately, we selected the chat observations as the method for taking player input. The .txt file was not selected because it requires the player to give input outside of the game and can cause errors, and the block approach was not selected because it is more work for the player and cannot be done from anywhere. The goal of this project is to make harvesting animal resources easier, and using chat messages proved to be the quickest option for the player, and can be done from anywhere on the *Minecraft* server.
#### Algorithms
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To determine the appropriate reinforcement learning algorithm for this project, we used an algorithm flowchart<sup>7</sup> which was provided as a course resource. We mainly tested two algorithms, one for discrete action space and one for continuous action space. This is because when we were deciding between action spaces, we wanted to make sure that both spaces were using an appropriate algorithm. For discrete action spaces, we selected the Proximal Policy Optimization (PPO) algorithm. We wanted the agent to be learning on-policy and we were not robust to hyperparameters. For continuous action spaces, we selected the Soft-Actor Critic (SAC) algorithm. Our environment was stochastic due to the random spawn locations and movements of animals, so SAC was the best fit. The algorithms used for BarnyardBot were off-the-shelf algorithms provided by RLlib.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ultimately, we determined that the discrete action space and RLlib PPO algorithm<sup>8</sup> showed better improvement than the continuous action space and RLlib SAC algorithm<sup>9</sup>. This confirmed our belief that choosing a discrete action space for the given problem was more appropriate than a continuous action space. The update equation for the off-the-shelf PPO algorithm is based on the ratio of old and new policies, and is roughly as follows:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$L^{CLIP}(\theta)=\hat{E_t}[min(r_t(\theta)\hat{A_t},clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})]$$&nbsp;&nbsp;&nbsp;<sup>10</sup>  <br/>
The off-the-shelf PPO algorithm in combination with the discrete action space, line of sight observation space, and ratios added to the observation space resulted in a model that could show improvement after a few hours of learning the current ratio. With this approach as the final choice, we were able to experiment with changing ratios during the missions and evaluate the resulting data.

## Evaluation
#### Quantitative
#### Qualitative

## References

#### Report References
[Minecraft<sup>1</sup>](https://www.minecraft.net/en-us/login)\
[Sheep Wool Statistic<sup>2</sup>](https://minecraft.fandom.com/wiki/Tutorials/Wool_farming)\
[Malmo<sup>3</sup>](https://www.microsoft.com/en-us/research/project/project-malmo/)\
[RLlib<sup>4</sup>](https://docs.ray.io/en/latest/rllib.html)\
[Gym<sup>5</sup>](https://gym.openai.com/)\
[Malmo XML<sup>6</sup>](https://microsoft.github.io/malmo/0.21.0/Schemas/MissionHandlers.html)\
[Reinforcement Learning Algorithm Flowchart<sup>7</sup>](https://static.us.edusercontent.com/files/eS20DbiGQfi4P2skbCN9WYeD)\
[RLlib PPO<sup>8</sup>](https://docs.ray.io/en/latest/rllib-algorithms.html#ppo)\
[RLlib SAC<sup>9</sup>](https://docs.ray.io/en/latest/rllib-algorithms.html#sac)\
[PPO Algorithm Source<sup>10</sup>](https://blogs.oracle.com/ai-and-datascience/post/reinforcement-learning-proximal-policy-optimization-ppo)
#### Other References
[Writing 3x3 Letters in Minecraft (For Home Page/Video Image)](https://www.youtube.com/watch?v=vHExVqV-FD8)\
[CS175 Assignment 2 for understanding of RLlib/Gym](https://canvas.eee.uci.edu/courses/40175/files/folder/assignment2?preview=16066666)
[Malmo Gitter Chat](https://gitter.im/Microsoft/malmo?at=578aa4fd3cb52e8b24cee1af)\
[Displaying Images Side by Side in GitHub](https://stackoverflow.com/questions/24319505/how-can-one-display-images-side-by-side-in-a-github-readme-md)



