---
layout: default
title: Final Report
---
## Final Report Video
<iframe width="720" height="405" src="https://www.youtube.com/embed/Pqerg-Vj71I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Project Summary
<img src="https://user-images.githubusercontent.com/51243475/144334516-68004b17-8994-4385-a4c4-7a60bf262afb.png" alt="Problem Image" width="425" height="300">  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gathering animal resources is a crucial part of *Minecraft*<sup>1</sup> that can often be tedious. When a player wants milk or wool for baking a cake, healing status effects, making beds, or building a new colorful project, they must find the correct tools to use and then track down the respective animal for harvesting. On average, it takes a minute for a sheep's wool to grow back<sup>2</sup> in *Minecraft*. This means that most of the time, the player must wait for the cooldown to end if they need a specific color of wool from a specific sheep. This makes animal resource harvesting time consuming, and forces the player to sit around and wait instead of exploring the world, mining, or working on a new build.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot solves this problem by automating the animal resource harvesting process. With the power of reinforcement learning, BarnyardBot can navigate throughout the animal pen and harvest resources for the player. The player can specify whether they need milk, a certain color of wool, or a ratio of resources. BaryardBot collects the requested items and gives the player more free time, removing the need to sit around and wait for cooldowns or chase around animals. This gives the player more time to explore the world, go mining for precious stones, cook food, or build their next work of art.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot takes the agent's current target and the player's requests as input and determines the current best solution for meeting the player's requests. For example, if the player only wants milk and does not want wool, BarnyardBot will find cows to milk and ignore the sheep it comes across. Taking the player's requests is important for solving the animal resource harvesting problem because the player won't always need every resource. BarnyardBot aims to use this information to harvest resources as efficiently and accurately as possible. If the player only needs a certain color of wool for a build, BarnyardBot won't waste time collecting other colors. Whenever the player wants a different resource, they can tell BarnyardBot and it will adjust accordingly. This prevents the player from needing multiple bots to perform a similar task, since BarnyardBot can harvest whatever they need from sheep and cows.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The goal of this project is to maximize resource output in *Minecraft* using *Malmo*<sup>3</sup> and *RLlib*<sup>4</sup>, with the output based on the requests of the player. Reinforcement learning was chosen for this project because the agent needs to learn which tools are used for which animals, and understand how this applies to the material ratio requested by the user. Additionally, the agent must learn to locate the animal that corresponds to the desired resources. Since we are trying to maximize resource output, reinforcement learning will help the agent reduce unnecessary actions and increase efficiency. BarnyardBot is applicable to the real world because the problem it solves is similar to packing orders, such as in a warehouse. A robot like the BarnyardBot agent would need to navigate the warehouse space and collect the requested number of each product or material, similar to how BarnyardBot navigates the animal pen and collects the resources desired by the user. Additionally, BarnyardBot learns how to collect different ratios of resources, and a warehouse robot would likely need to collect different items for each order.

## Approaches
#### Setup
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BarnyardBot uses reinforcement learning through *RLlib*, *Gym*<sup>5</sup>, and *Malmo* to adjust agent behavior in *Minecraft* for harvesting resources. To set up the environment, we spawn an 11 by 11 block perimeter fence in a flat world, and summon 8 cows, 8 red sheep, and 8 blue sheep. All animals are summoned at random locations inside the pen. We found that summoning the animals through *Minecraft* summon chat commands was better than using the *Malmo* XML<sup>6</sup> because it gave us more freedom for summoning animals at other times and customizing colors and nametags. The agent spawns in the middle of the pen, and has shears in hotbar 1 and a bucket in hotbar 2 so it can interact with the animals. The agent learns in 30 second missions, and the environment resets after each mission. We found this to be effective because it regularly randomizes the locations of the animals around the agent. It also minimizes the chances of the agent picking up wool that it sheared previously and missed, which was important because it encourages the agent to pick up harvested materials to score points. The first challenge we came across when setting up the environment was that harvesting milk with a bucket did not trigger the *Malmo* reward system. To fix this, we used ‘ObservationFromHotbar’ in the *Malmo* XML. Whenever a milk bucket was observed in the agent's hand, we used *Minecraft* chat commands to replace the milk bucket with a regular bucket. We then incremented the total reward by the appropriate amount for milk.  <br/>
#### Actions and Observations
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When creating the first baseline for the project, the most important details were how the agent would perform its actions (the action space) and how the agent would observe its environment (the observation space). We tested two different action spaces: continuous and discrete. For the continuous action space, the agent could turn left or right, walk forwards or backwards, switch between hotbar 1 and 2, and use the current item. For the discrete action space, the agent could move forwards, backwards, left, or right, switch between hotbar 1 and 2, and use the current item. One of the challenges for the discrete action space was that using an item through Malmo didn't work. To fix this, we used both continuous and discrete commands, and used continuous whenever the agent tried to use an item. This was done utilizing the command 'allow-list' and 'deny-list' options in the Malmo XML. One of the challenges of using a continuous use command was that the agent could infinitely milk the same cow when its bucket was replaced to reward for collecting milk. To fix this, we kill and respawn all cows using the ‘/kill @e[type=minecraft:cow]’ chat command.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Overall, we found that the discrete action space with a continuous use command worked best for our project because the agent could quickly navigate the space and locate new animals. With continuous actions, the agent spent a lot of time spinning and had a harder time harvesting resources. Because the animal pen is an 11 by 11 block grid, discrete actions were also useful for ensuring the agent stayed on the grid. Another issue with the continuous action space was the amount of data needed for the agent to learn. Because there are so many possible actions, the agent would need to spend much more time testing and exploring the possibilities, and we found that running the program for several hours was not enough to see improvement.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We tested two different observation spaces: a 5 by 5 block grid of entities around the agent, and a number representing the entity in the line of sight of the agent. For both approaches, air was represented as 0, cows were represented as 1, red sheep were represented as 2, blue sheep were represented as 3, and the agent was represented as 4. For the 5 by 5 grid approach, we used ‘ObservationFromNearbyEntities’ in the *Malmo* XML. This returned the location of all entities, and we created a matrix based on the blocks surrounding the agent. It was important to account for the Agent's YAW and adjust the grid based on the direction it was facing. The main issue for this approach was the large number of possible observation states. Because each of the 25 entries in the matrix had 5 possibilities, there were 5<sup>25</sup> possible states and the agent struggled to learn with any algorithm. Learning over this observation space would take a very long time, as the agent would need to explore all of the different possible states and test every action over these states. Also, because there are so many possible states, some of them would never be encountered after running the program overnight. For the line of sight approach, we used ‘ObservationFromRay’ in the *Malmo* XML. This returned the type of entity in the agent's line of sight, which was converted into a tuple (x,y) where x represented the entity number and y represented the current hotbar slot (1 or 2). This approach has 4 \* 2 possibilities, which was much more manageable for the learning algorithm. Because less data was needed for the learning algorithm to show improvements, we decided to use the line of sight approach for the base observation.  <br/>
#### Ratios
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The base observation space was then expanded to include player input in the form of ratios. These ratios represent what items the player wants the agent to harvest, and are in the format m:r:b where m is the number for milk, r is the number for red wool, and b is the number for blue wool. Each number can be between 0 and 2 inclusive. For example, 1:1:1 means equal resources of each, 1:0:0 means harvest milk and no wool, and 2:1:0 means harvest 2 milk for every red wool, and no blue wool. These numbers are then added to the observation space (one for each item, values 0, 1, or 2), and the reward for collecting each resource is adjusted accordingly. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also experimented with more complex ratios by either adding in penalties for having mismatched ratios which is checked on reset, or giving each target item a non zero reward value equivalent to its proportion with respect to the other items (for instance, wanting 2/3 milk would result in milk being worth 0.66 points), or both. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When an item has a 0 ratio, it should return -1 reward instead of 0 to penalize the agent for collecting it. This increased the number of possibilities in the observation space from 4 \* 2 to 4 \* 2 \* 3 \* 3 \* 3, or 216 possibilities. For example, [1,1,0,0,1] means the agent is looking at a cow, holding shears, and the current request is to only collect blue wool. The increase in observation space required more time in the learning process, but increased the complexity of the observations.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Several approaches were taken for implementing ratios into the observation space. The first approach used a .txt file where the player would type the desired ratio of items on line 3. The program would read the .txt file at the start of every mission and adjust the observations and rewards accordingly. This approach was the simplest, as the player just had to open the .txt file and make an adjustment whenever they felt necessary. One of the issues is that the player can cause an error if they are editing the file while the program is trying to read it. To avoid this, the player needs to pay attention to when the mission is resetting.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another approach involved using ‘ObservationFromChat’ in the *Malmo* XML. With this approach, the program reads any chat messages in the format ‘!RATIO m:r:b’ and adjusts the observations and rewards accordingly. This approach is useful because it allows the player to adjust the agent’s current goals from inside of *Minecraft*, rather than having to edit a file in a separate program. One of the issues with this approach was that *Malmo* occasionally misses chat messages, as is explained in the *Malmo* XML. To combat this, the agent sends a chat command saying ‘Ratio m:r:b accepted, and will be used starting in the next mission’ whenever a chat message is successfully read. If the agent doesn’t respond, the player knows that they have to send the chat message again. Sometimes it can take 3 or so tries before the observation is triggered by *Malmo*.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The last approach used ‘ObservationFromGrid’ in the *Malmo* XML. With this approach, the program detects a grid of blocks placed by the player that represent the ratio. Each resource is represented by a different block, and the rewards and observations are adjusted accordingly. One challenge with this method is that the agent is constantly moving, so the observation grid has to be large. To fix this, we parsed the observation grid for the three types of blocks used and entered the appropriate ratio into the observation. Ultimately, we selected the chat observations as the method for taking player input. The .txt file was not selected because it requires the player to give input outside of the game and can cause errors, and the block approach was not selected because it is more work for the player and cannot be done from anywhere. The goal of this project is to make harvesting animal resources easier, and using chat messages proved to be the quickest option for the player, and can be done from anywhere on the *Minecraft* server.
#### Algorithms
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To determine the appropriate reinforcement learning algorithm for this project, we used an algorithm flowchart<sup>7</sup> which was provided as a course resource. We mainly tested two algorithms, one for discrete action space and one for continuous action space. This is because when we were deciding between action spaces, we wanted to make sure that both spaces were using an appropriate algorithm. For discrete action spaces, we selected the Proximal Policy Optimization (PPO) algorithm. We wanted the agent to be learning on-policy and we were not robust to hyperparameters. For continuous action spaces, we selected the Soft-Actor Critic (SAC) algorithm. Our environment was stochastic due to the random spawn locations and movements of animals, so SAC was the best fit. The algorithms used for BarnyardBot were off-the-shelf algorithms provided by RLlib.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ultimately, we determined that the discrete action space and RLlib PPO algorithm<sup>8</sup> showed better improvement than the continuous action space and RLlib SAC algorithm<sup>9</sup>. This confirmed our belief that choosing a discrete action space for the given problem was more appropriate than a continuous action space. The update equation for the off-the-shelf PPO algorithm is based on the ratio of old and new policies, and is roughly as follows:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$L^{CLIP}(\theta)=\hat{E_t}[min(r_t(\theta)\hat{A_t},clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})]$$&nbsp;&nbsp;&nbsp;<sup>10</sup>  <br/>
The off-the-shelf PPO algorithm in combination with the discrete action space, line of sight observation space, and ratios added to the observation space resulted in a model that could show improvement after a few hours of learning the current ratio.<br/>

<img src="https://user-images.githubusercontent.com/50119976/145662204-98c9dbc8-7a75-417a-9d7e-484dfde57441.png"> <br/>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additionally, the effect of the learning rate hyperparameter was explored to be able to tune it for better results. We experimented with different magnitudes of learning rate and recorded its affect on the rewards earned. The results in the graph above show that the default learning rate provided by RLlib was the most optimal of the choices. However, more testing within the default magnitude could potentially yield a more optimal learning rate for the trainer. As such we can use this method to tune other hyperparameters, like train_batch_size, in order to find a combination that would give the most points. Unfortunatly this would many more hours to manually find and tune each parameter and additional hardware to accelerate the process as we did not use GPU acceleration.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With the *RLlib* PPO algorithm as the final choice, we were able to experiment with changing ratios during the missions and evaluate the resulting data.

## Evaluation
#### Quantitative
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quantitative evaluations for BarnyardBot are based around two statistics, reward per 30 second mission and number of resources collected per 30 second mission. For reward, the goal was to maximize the points scored by BarnyardBot during each mission. This requires the algorithm to learn which actions earn points, and how to do so efficiently. For resources collected, the goal was to fit the number of resources collected to the ratio collected by the user. This requires the algorithm to take the ratio requested by the user as an observation and learn to follow it effectively. The following charts show average resources collected per step with different requested ratios.
<p float=left>
  <img src="https://user-images.githubusercontent.com/51243475/141702221-252f1a85-ed36-4342-899f-e2ef6647baf4.png" alt="Milk Only" width="300" height="220">
  <img src="https://user-images.githubusercontent.com/51243475/141702224-9894a05a-2b22-4b2a-b3d3-3541ed599539.png" alt="Wool Only" width="300" height="220">
  <img src="https://user-images.githubusercontent.com/51243475/141702227-fa5cf5e6-7ab8-4fb0-8e2e-1e9ae1ee0286.png" alt="Equal of All" width="300" height="220">
</p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The ratios evaluated for total reward were collecting milk only (1:0:0 ratio), collecting equal parts of red and blue wool only (0:1:1 ratio), and collecting equal amounts of all resources (1:1:1 ratio). As can be seen in the figures above, all three reward ratios resulted in a general uptrend in reward per mission over many steps. The three graphs show generally linear growth on average, and none of them appear to have reached a ceiling. One standout in the third graph, with equal points for each resource, is the spike in reward around 7,500 to 8,500 steps. This could be due to the agent temporarily finding a way to earn many points, such as cornering animals, that eventually stopped working and resulted in less points. It is also possible that a few outlier missions with high reward (15+ points) unevenly weighted the average for these steps. This could be due to the randomized movement of animals concentrating them around the agent, or the random number generator that regrows sheep wool being lucky. Regardless, it can be seen that the overall reward was trending back towards those numbers before the agent was stopped around 22,000 steps. The graphs of average reward per ten steps prove that the PPO algorithm for reinforcement learning was working effectively to teach the agent to gather resources. Positive average slope in all three graphs also means that regardless of the selected ratio (1:0:0, 0:1:1, 1:1:1), the BarnyardBot was working correctly and returning the expected result. When other algorithms were tested, such as SAC and DQN, they did not have similar results and instead reflected very little learning. This supports that the PPO algorithm is the correct choice for BarnyardBot, and that BarnyardBot was able to improve its animal finding and harvesting ability with PPO reinforcement learning.  <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another aspect of these charts that can be used to evaluate BarnyardBot is the average points over different step ranges. For the first 5000 steps of each run, for example, the average points scored per mission was 0.47 for collecting milk only, 0.22 for collecting equal red and blue wool, and 2.15 for collecting equal of all resources. A higher average reward for collecting all resources makes sense, because there were no resources that resulted in a negative reward. This means the agent had to spend less time learning what was good and bad, and more time learning how to locate and collect the right resources for each animal. Over the last 5000 steps of each run, the average was 4.42 for milk only, 2.38 for wool only, and 5.28 for collecting all resources. That makes the improvements over time +3.95 for collecting milk, +2.16 for collecting red and blue wool, and +3.13 for collecting all resources. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
The positive slope in the graphs and values for difference in average points scored reflects that the learning algorithm has a significant impact on agent performance. Additionally, it can be assumed from the graphs that spending more time training the agent will result in a higher increase in resources collected, since the slopes did not level off towards the end. If the mission lengths were increased from 30 seconds to a longer time, such as 60 seconds, the agent would also collect more resources and rewards per mission. The average values 4.42, 2.38, and 5.28 for milk only, red and blue wool only, and all resources, mean that if the agent was run for 10 minutes it would collect 88 buckets of milk if only collecting milk, 48 wool if only collecting wool, or 106 items if collecting everything. These numbers are all useful amounts considering that the player could let the agent do all of the work. However, the number for only collecting wool for 10 minutes would ideally be higher than 48. One reason this number may be lower than the moonshot goal could be that the agent attempts to shear already-sheared sheep. For future versions of the project, one goal could be to increase this number through identification of ready and unready sheep.

<p float=left>
  <img src="https://user-images.githubusercontent.com/51243475/144724999-52e97648-326a-4373-8827-b09c17eda1ab.png" alt="Changing Ratio While Learning" width="450" height="330">
</p>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The next test for total reward was switching the ratio while BarnyardBot was learning. This test is important because it shows that the agent is able to switch the target resource while running, rather than needing to restart the agent. This allows for the user to have one agent that can harvest any resource, rather than multiple agents for multiple resources.. The target ratio for BarnyardBot was initially set to collecting milk only (1:0:0), then changed to collecting equal parts red and blue wool (0:1:1), then set back to collecting milk only again (1:0:0). Ratio changes were done at the 15,000 step mark and 30,000 step mark, and after that the agent was left alone to harvest and show improvement until about 75,000 steps. The incline slope of the graph during the first segment suggests that the agent was successfully learning how to collect only milk, as the average return went from between 0 and -1 to over 6. When the ratio was changed, the average return dropped to almost -6 because the agent was now being penalized for collecting milk, and the algorithm needed to adjust. The positive slope in the second segment and change in average reward from almost -6 back to positive values suggest that BarnyardBot adjusted to the new ratio as expected. At the 30,000 step mark, the positive slope continues but is not as large as in the previous round, growing from just above 0 to around 4 average reward. The smaller, positive slope suggests that the agent was able to remember what it learned in the first 1:0:0 ratio segment and adjust accordingly, and it still remembered what it learned from the 0:1:1 ratio as well. Once again, the positive slope around the 75,000 step mark suggests that the agent would continue to increase the number of resources collected if it was run for a longer period of time.


<p float=left>
  <img src="https://user-images.githubusercontent.com/51243475/144723703-10c345d3-11f8-4c69-89b2-8bdf8b8a00bf.png" alt="1:0:0 Ratio Resources" width="375" height="275">
  <img src="https://user-images.githubusercontent.com/51243475/144723701-c6321885-442a-4e55-9c04-63bf487d1c19.png" alt="0:1:1 Ratio Resources" width="375" height="275">
</p>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The above graphs show the number of each type of resource collected over time, with different ratios as the initial target. For the first graph, the agent was given a 1:0:0 ratio, meaning to only collect milk. The black line represents milk collected, red line represents red wool collected, and blue line represents blue wool collected. By analyzing the slopes of the graph, it can be determined that milk has a positive slope, and red and blue wool settle to a slope of 0 around the 0 materials collected mark. Slight bumps in the red and blue lines show that the agent occasionally collects a red or blue wool, but consistently collects higher amounts of milk. The data used to construct the graphs shows that over the first 129 steps of the run, the difference in milk and red wool collected was 0 on average. The difference in milk and blue wool collected was -3 on average, meaning more blue wool was being collected than milk. At 18,998 steps, the differences increased to 10 and 9 respectively. The graphs do not show numbers this high because the points are plotted as smoothed values for every 10 missions, which is why the average for milk is seen just above 6. These numbers all suggest that the algorithm is working as expected, as the milk collected is increasing and the ratio of 1:0:0 is being followed. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The second graph is for a ratio of 0:1:1, where the agent was supposed to collect equal amounts of red and blue wool, and collect no milk. As can be seen by the graph, the red and blue lines representing red and blue wool collected stay close in value, with the average difference between materials collected being 0.173. Since the materials are collected in increments of 1, it takes 5.88 missions on average for there to be a difference in red and blue wool collected. The black line representing milk collected in this graph falls to a slope of near 0 around the 0 materials harvested line. In the first half of the missions, the average milk collected per mission was 0.91 buckets, the average red wool collected was 1.28 blocks, and the average blue wool collected was 1.51 blocks. In the second half of missions, the average milk collected was 0.11 buckets, the average red wool was 2.53 blocks, and the average blue wool was 2.54 blocks. Once again this shows that the agent was able to adhere to the ratio requested and improve performance through training.

#### Qualitative
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first qualitative evaluations for BarnyardBot are whether or not it collected a reasonable amount of resources, and whether this amount would be useful. After about an hour of collecting milk, for example, BarnyardBot averaged six buckets of milk per thirty seconds. This can be considered reasonable, since the bot has to track down six cows and milk them individually. If a user were to leave BarnyardBot running for ten minutes while they worked on a different task, they could expect to have 120 buckets of milk waiting for them. Given that a cake in *Minecraft* only uses three buckets of milk, and healing status effects only uses one bucket of milk, the amount of resources collected can be considered reasonable. With BarnyardBot, it is unlikely that the player would ever find themselves needing buckets of milk and not having them. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For wool, it is a similar story. In the test runs, BarnyardBot was collecting both red and blue wool. After about an hour of collecting both colors, BarnyardBot was averaging just over three of each wool per thirty second mission. This can be considered reasonable because the agent was collecting two different colors of wool, for about six total wool per mission. This averages to the same amount of blocks as milk buckets collected. In a ten minute time span, the agent would collect sixty of each type of wool, which is almost a full stack in *Minecraft*, or 120 total. If the user was crafting beds, which require three wool each, or carpets, which require two wool each, BarnyardBot would be collecting plenty of wool for the user. Additionally, the agent was occasionally harvesting wool from all of the available specified sheep and needed to wait for wool to grow back. If the user was harvesting wool themselves, they would run into the same stopping point. If the user was working on a build with wool, they would need to place a piece of wool every five seconds to keep up with BarnyardBot. If the player knows exactly where each block needs to go, it could be argued that BarnyardBot needs to harvest wool faster. However, building in *Minecraft* takes time and thought, and there are many blocks to build with. It is not uncommon for a player to use multiple blocks together in a build, such as wood, stone, and wool. Because of this, a block of wool every five seconds on average is not unreasonable, and would be useful to the user.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another qualitative evaluation for BarnyardBot is whether or not switching the requested ratio worked. Overall, the answer to this question is yes. The three methods tested, .txt files, chat messages, and block placement, all successfully updated the parameters and rewards for BarnyardBot. After the parameters were switched, the ratio of resources collected made it clear that the same agent could be used for collecting different materials. This is useful to the user because it allows them to use only one agent rather than needing multiple agents in multiple pens. If switching ratios was not successful, the agent would not avoid specific materials after long periods of reinforcement learning. It is understandable that the agent will still occasionally make a mistake with harvesting an incorrect color of wool or bucket of milk after a few hours of learning. The current version is still quite accurate and collects reasonable amounts of resources, but a version of the project that reaches the moonshot goal would make zero mistakes. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The moonshot goal for this project was to decrease all unnecessary actions, and create an agent that would harvest animal resources as efficiently as possible. After running the learning algorithm overnight, however, we found that the agent was always improving its performance. Even if it was a slight increase, the slope was still positive. Because of this, we know that the agent didn’t reach the moonshot goal of being as efficient as possible. If it had, it would reach a point where it could no longer improve. Additionally, the moonshot goal for this project would have reached that point earlier than it took in practice. One potential cause for unnecessary actions could be that wool is dropped in random directions in *Minecraft*. For example, when the agent shears a sheep, it cannot be determined whether the wool will be dropped in front of, behind, or to the left or right of the sheep. This makes it harder for BarnyardBot to maximize efficiency because it cannot predict which direction it needs to move in to collect the wool it harvested. Instead, the agent would often shear a sheep and spend a few seconds moving around the area until it received a reward. Despite not reaching the moonshot goal, it is clear that BarnyardBot was still able to accomplish the task at hand. It passed both of the other qualitative evaluations, and showed clear improvement in the data and graphs from the quantitative evaluations. 

## References

#### Report References
[Minecraft<sup>1</sup>](https://www.minecraft.net/en-us/login)\
[Sheep Wool Statistic<sup>2</sup>](https://minecraft.fandom.com/wiki/Tutorials/Wool_farming)\
[Malmo<sup>3</sup>](https://www.microsoft.com/en-us/research/project/project-malmo/)\
[RLlib<sup>4</sup>](https://docs.ray.io/en/latest/rllib.html)\
[Gym<sup>5</sup>](https://gym.openai.com/)\
[Malmo XML<sup>6</sup>](https://microsoft.github.io/malmo/0.21.0/Schemas/MissionHandlers.html)\
[Reinforcement Learning Algorithm Flowchart<sup>7</sup>](https://static.us.edusercontent.com/files/eS20DbiGQfi4P2skbCN9WYeD)\
[RLlib PPO<sup>8</sup>](https://docs.ray.io/en/latest/rllib-algorithms.html#ppo)\
[RLlib SAC<sup>9</sup>](https://docs.ray.io/en/latest/rllib-algorithms.html#sac)\
[PPO Algorithm Source<sup>10</sup>](https://blogs.oracle.com/ai-and-datascience/post/reinforcement-learning-proximal-policy-optimization-ppo)
#### Other References
[UCI Minecraft Server (used in video)](https://sites.uci.edu/minecraft/)\
[Writing 3x3 Letters in Minecraft (For Home Page/Video Image)](https://www.youtube.com/watch?v=vHExVqV-FD8)\
[CS175 Assignment 2 for understanding of RLlib/Gym](https://canvas.eee.uci.edu/courses/40175/files/folder/assignment2?preview=16066666)\
[Malmo Gitter Chat](https://gitter.im/Microsoft/malmo?at=578aa4fd3cb52e8b24cee1af)\
[Displaying Images Side by Side in GitHub](https://stackoverflow.com/questions/24319505/how-can-one-display-images-side-by-side-in-a-github-readme-md)\
[Image Resizing Resolution Calculator](https://red-route.org/code/image-resizing-calculator)
